{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  Library\n",
    "# ===================================================================\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from math import comb\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import unicodedata\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  CFG\n",
    "# ===================================================================\n",
    "class CFG:\n",
    "    filename = \"exp040\"\n",
    "    seed = 42\n",
    "    n_splits = 8\n",
    "    data_dir = \"G:/マイドライブ/signate_StudentCup2023/data/\"\n",
    "    num_boost_round = 10000\n",
    "    stopping_rounds = 100\n",
    "    save_dir = \"G:/マイドライブ/signate_StudentCup2023/exp/\"\n",
    "    num_cores = 4 # kaggleの方と統一\n",
    "    categorical_features = [\n",
    "        \"fuel\", \"title_status\", \"type\", \"state\", \"region\", \"manufacturer\", \"condition\", \"cylinders\", \"transmission\", \"drive\", \"size\", \"paint_color\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  Utils\n",
    "# ===================================================================\n",
    "def seed_everything(seed):\n",
    "    \"\"\"fix random factors\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "seed_everything(CFG.seed)\n",
    "    \n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    \"\"\"get MAPE score\"\"\"\n",
    "    score = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return score * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  Data Loading\n",
    "# ===================================================================\n",
    "train = pl.read_csv(CFG.data_dir+\"train.csv\")\n",
    "test = pl.read_csv(CFG.data_dir+\"test.csv\")\n",
    "\n",
    "region_coor = pl.read_csv(CFG.data_dir+\"region_coordinate.csv\")\n",
    "state_coor = pl.read_csv(CFG.data_dir+\"state_coordinate.csv\")\n",
    "\n",
    "train = train.with_columns(pl.lit(\"train\").alias(\"flag\"))\n",
    "test = test.with_columns(\n",
    "    [\n",
    "        pl.lit(None, dtype=pl.Int64).alias(\"price\"),\n",
    "        pl.lit(\"test\").alias(\"flag\"),\n",
    "     ])\n",
    "all_data = pl.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  feature_engineering\n",
    "# ===================================================================\n",
    "def preprocessing(all_data: pl.DataFrame) ->pl.DataFrame:\n",
    "    \"\"\"\n",
    "    train, testデータで共通の前処理のコード\n",
    "    \n",
    "    ・yearの異常値を直す\n",
    "    ・manufacturerの表記を統一する\n",
    "    ・sizeの表記を統一する\n",
    "    ・regionの欠損値をtrain dataの(state, region)の組み合わせから補完する。残った欠損値は調べて補完する。\n",
    "    ・title_statusとtypeの欠損値処理はとりあえず放置\n",
    "    ・year関係の特徴量を加える\n",
    "\n",
    "    Args:\n",
    "        all_data (pl.DataFrame): pl.concat([train, test])\n",
    "    \"\"\"\n",
    "    # year\n",
    "    year_dict = {\n",
    "        2999:1999,\n",
    "        3008:2008,\n",
    "        3011:2011,\n",
    "        3015:2015,\n",
    "        3017:2017,\n",
    "        3019:2019,\n",
    "    }\n",
    "    all_data = all_data.with_columns(\n",
    "        pl.when(pl.col(\"year\").is_in(list(year_dict.keys())))\n",
    "        .then(pl.col(\"year\").apply(lambda x: year_dict.get(x)))\n",
    "        .otherwise(pl.col(\"year\"))\n",
    "        .alias(\"year\")\n",
    "    )\n",
    "    \n",
    "    # manufacturer\n",
    "    all_data = all_data.with_columns(\n",
    "        pl.col(\"manufacturer\").apply(lambda x: unicodedata.normalize('NFKC', x.lower())).alias(\"manufacturer\")\n",
    "    )\n",
    "    manufacturer_map = {\n",
    "        'niѕsan':'nissan',\n",
    "        'nisѕan':'nissan',\n",
    "        'subαru':'subaru',\n",
    "        'toyotа':'toyota',\n",
    "        'sαturn':'saturn',\n",
    "        'аcura':'acura',\n",
    "        'vоlkswagen':'volkswagen',\n",
    "        'lexuѕ':'lexus',\n",
    "        'ᴄhrysler':'chrysler',\n",
    "    }\n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"manufacturer\").is_in(list(manufacturer_map.keys())))\n",
    "            .then(pl.col(\"manufacturer\").apply(lambda x: manufacturer_map.get(x)))\n",
    "            .otherwise(pl.col(\"manufacturer\"))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # size\n",
    "    size_dict = {\n",
    "        \"fullーsize\":\"full-size\",\n",
    "        \"midーsize\":\"mid-size\",\n",
    "        \"subーcompact\":\"sub-compact\",\n",
    "        \"full−size\":\"full-size\",\n",
    "        \"mid−size\":\"mid-size\"\n",
    "    }\n",
    "    all_data = all_data.with_columns([\n",
    "        pl.when(pl.col(\"size\").is_in(list(size_dict.keys())))\n",
    "        .then(pl.col(\"size\").apply(lambda x: size_dict.get(x)))\n",
    "        .otherwise(pl.col(\"size\"))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # region\n",
    "    region_state = {region:{} for region in all_data.filter(pl.col(\"flag\") == \"train\").select(pl.col(\"region\")).unique().get_columns()[0]}\n",
    "    for row in all_data.filter(pl.col(\"flag\") == \"train\").select(pl.col(\"state\", \"region\")).rows():\n",
    "        if row[0] is not None:\n",
    "            if row[0] not in region_state[row[1]]:\n",
    "                region_state[row[1]][row[0]] = 1\n",
    "            else:\n",
    "                region_state[row[1]][row[0]] += 1\n",
    "    for region, state_dict in region_state.items():\n",
    "        if len(state_dict) > 1 or state_dict == {}:\n",
    "            region_state[region] = np.nan\n",
    "        else:\n",
    "            region_state[region] = list(state_dict.keys())[0]\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"state\").is_null())\n",
    "            .then(pl.col(\"region\").apply(lambda x: region_state.get(x)))\n",
    "            .otherwise(pl.col(\"state\"))\n",
    "            .alias(\"state\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        pl.when(pl.col(\"region\") == \"northwest KS\").then(pl.lit('ks'))\n",
    "        .when(pl.col(\"region\") == \"ashtabula\").then(pl.lit('oh'))\n",
    "        .when(pl.col(\"region\") == \"southern WV\").then(pl.lit('wv'))\n",
    "        .otherwise(pl.col(\"state\"))\n",
    "        .alias(\"state\")\n",
    "    )\n",
    "    \n",
    "    # 緯度経度\n",
    "    all_data = all_data.join(region_coor, on=\"region\", how=\"left\")\n",
    "    all_data = all_data.join(state_coor, on=\"state\", how=\"left\")\n",
    "    \n",
    "    # type\n",
    "    ## 欠損値 train: 456, test: 229\n",
    "    \n",
    "    # title_status\n",
    "    ## 欠損値 train: 456, test: 229\n",
    "    \n",
    "    # fuel\n",
    "    ## 欠損値 train: 1239, test: 1495\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            (2023 - pl.col(\"year\")).alias(\"elapsed_years\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            pl.col(\"elapsed_years\").log().alias(\"log_elapsed_years\"),\n",
    "            pl.col(\"elapsed_years\").sqrt().alias(\"sqrt_elapsed_years\"),\n",
    "        ]\n",
    "    )\n",
    "    return all_data\n",
    "all_data = preprocessing(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance of the mean of the folds:  7.232164844021038\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  Cross Validation\n",
    "# ===================================================================\n",
    "train = all_data.filter(pl.col(\"flag\") == \"train\")\n",
    "test = all_data.filter(pl.col(\"flag\") == \"test\")\n",
    "\n",
    "train = train.sort(by=\"price\")\n",
    "train = train.with_columns(\n",
    "    [\n",
    "        pl.Series([i for i in range(CFG.n_splits)] * (train.shape[0] // CFG.n_splits) + [i for i in range(train.shape[0] % CFG.n_splits)]).alias(\"fold\")\n",
    "    ]\n",
    ")\n",
    "train.sort(by=\"id\")\n",
    "print(\"The variance of the mean of the folds: \", train.groupby(\"fold\").agg(pl.col(\"price\").mean()).get_columns()[1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_per_fold(CFG, train:pd.DataFrame, test: pd.DataFrame,  fold: int = 0):\n",
    "    \"\"\"foldごとの前処理: leakageを防ぐ\n",
    "\n",
    "    Args:\n",
    "        CFG :config\n",
    "        train (pd.DataFrame): 学習データ\n",
    "        test (pd.DataFrame, optional): test data Defaults to None.\n",
    "        fold (int, optional): Defaults to 0.\n",
    "    \"\"\"\n",
    "    X_train = train.filter(\n",
    "        pl.col(\"fold\") != fold\n",
    "    )\n",
    "    X_valid = train.filter(\n",
    "        pl.col(\"fold\") == fold\n",
    "    )\n",
    "    test_df = test.clone()\n",
    "    \n",
    "    fillna_map = X_train.filter(\n",
    "        (100 < pl.col(\"odometer\")) | (pl.col(\"odometer\") < 400_000) \n",
    "    ).groupby(\"region\").agg(pl.col(\"odometer\").mean())\n",
    "    \n",
    "    def replace_odometer(df: pl.DataFrame, fillna_map: pl.DataFrame)-> pl.DataFrame:\n",
    "        df_1 = df.filter(\n",
    "            (pl.col(\"odometer\") < 100) | (pl.col(\"odometer\") > 400_000)\n",
    "        )\n",
    "        df_2 = df.filter(\n",
    "            (pl.col(\"odometer\") > 100) & (pl.col(\"odometer\") < 400_000)\n",
    "        )\n",
    "        df_1 = df_1.drop(\"odometer\")\n",
    "        df_1 = df_1.join(fillna_map, on=\"region\", how=\"left\")\n",
    "        df_2 = df_2.with_columns(\n",
    "            pl.col(\"odometer\").cast(pl.Float64)\n",
    "        )\n",
    "        df = pl.concat([df_1, df_2], how=\"diagonal\")\n",
    "        return df\n",
    "    X_train = replace_odometer(X_train, fillna_map)\n",
    "    X_valid = replace_odometer(X_valid, fillna_map)\n",
    "    test_df = replace_odometer(test_df, fillna_map)\n",
    "\n",
    "    odometer_mean = X_train[\"odometer\"].mean()\n",
    "\n",
    "    X_train = X_train.with_columns(pl.col(\"odometer\").fill_nan(odometer_mean))\n",
    "    X_valid = X_valid.with_columns(pl.col(\"odometer\").fill_nan(odometer_mean))\n",
    "    test_df = test_df.with_columns(pl.col(\"odometer\").fill_nan(odometer_mean))\n",
    "    \n",
    "    \n",
    "    def apply_fe(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = df.with_columns(\n",
    "            [\n",
    "                pl.col(\"odometer\").log().alias(\"log_odometer\"),\n",
    "                pl.col(\"odometer\").sqrt().alias(\"sqrt_odometer\"),\n",
    "                (pl.col(\"elapsed_years\") * pl.col(\"odometer\")).alias(\"elapsed_years*odometer\"),\n",
    "                (pl.col(\"elapsed_years\") * pl.col(\"odometer\").log()).alias(\"elapsed_years*log_odometer\"),\n",
    "                (pl.col(\"elapsed_years\") * pl.col(\"odometer\").sqrt()).alias(\"elapsed_years*sqrt_odometer\"),\n",
    "                (pl.col(\"elapsed_years\").log() * pl.col(\"odometer\")).alias(\"log_elapsed_years*odometer\"),\n",
    "                (pl.col(\"elapsed_years\").log() * pl.col(\"odometer\").log()).alias(\"log_elapsed_years*log_odometer\"),\n",
    "                (pl.col(\"elapsed_years\").log() * pl.col(\"odometer\").sqrt()).alias(\"log_elapsed_years*sqrt_odometer\"),\n",
    "                (pl.col(\"elapsed_years\").sqrt() * pl.col(\"odometer\")).alias(\"sqrt_elapsed_years*odometer\"),\n",
    "                (pl.col(\"elapsed_years\").sqrt() * pl.col(\"odometer\").log()).alias(\"sqrt_elapsed_years*log_odometer\"),\n",
    "                (pl.col(\"elapsed_years\").sqrt() * pl.col(\"odometer\").sqrt()).alias(\"sqrt_elapsed_years*sqrt_odometer\"),\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        return df\n",
    "    X_train = apply_fe(X_train)\n",
    "    X_valid = apply_fe(X_valid)\n",
    "    test_df = apply_fe(test_df)\n",
    "    \n",
    "    \n",
    "    for col in CFG.categorical_features:\n",
    "        count_map = X_train[col].value_counts().rename({\"counts\":col+\"_count_encoding\"})\n",
    "        X_train = X_train.join(count_map, on=col, how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=col, how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=col, how=\"left\")\n",
    "        \n",
    "        \n",
    "    for col in CFG.categorical_features:\n",
    "        fillna_map =  X_train.groupby(col).agg(\n",
    "            pl.col(\"price\").mean().alias(col+\"_mean_encoding\"),\n",
    "            pl.col(\"price\").std().alias(col+\"_std_encoding\"),\n",
    "            pl.col(\"price\").max().alias(col+\"_max_encoding\"),\n",
    "            pl.col(\"price\").min().alias(col+\"_min_encoding\"),\n",
    "            pl.col(\"price\").median().alias(col+\"_median_encoding\"),\n",
    "            \n",
    "            pl.col(\"odometer\").mean().alias(col+\"_mean_odometer_encoding\"),\n",
    "            pl.col(\"odometer\").std().alias(col+\"_std_odometer_encoding\"),\n",
    "            pl.col(\"odometer\").max().alias(col+\"_max_odometer_encoding\"),\n",
    "            pl.col(\"odometer\").min().alias(col+\"_min_odometer_encoding\"),\n",
    "            pl.col(\"odometer\").median().alias(col+\"_median_odometer_encoding\"),\n",
    "            #(pl.col(\"odometer\").median() - pl.col(\"odometer\")).alias(col+\"_median_odometer_encoding_diff\"),\n",
    "            #(pl.col(\"odometer\").mean() - pl.col(\"odometer\")).alias(col+\"_mean_odometer_encoding_diff\"),\n",
    "            \n",
    "            pl.col(\"year\").mean().alias(col+\"_mean_elapsed_years_encoding\"),\n",
    "            pl.col(\"year\").std().alias(col+\"_std_elapsed_years_encoding\"),\n",
    "            pl.col(\"year\").max().alias(col+\"_max_elapsed_years_encoding\"),\n",
    "            pl.col(\"year\").min().alias(col+\"_min_elapsed_years_encoding\"),\n",
    "            pl.col(\"year\").median().alias(col+\"_median_elapsed_years_encoding\"),\n",
    "            #(pl.col(\"year\").median() - pl.col(\"year\")).alias(col+\"_median_year_encoding_diff\"),\n",
    "            #(pl.col(\"year\").mean() - pl.col(\"year\")).alias(col+\"_mean_year_encoding_diff\"),\n",
    "            )\n",
    "        X_train = X_train.join(fillna_map, on=col, how=\"left\")\n",
    "        X_train = X_train.with_columns(\n",
    "            [\n",
    "                (pl.col(col+\"_median_odometer_encoding\") - pl.col(\"odometer\")).alias(col+\"_median_odometer_encoding_diff\"),\n",
    "                (pl.col(col+\"_mean_odometer_encoding\") - pl.col(\"odometer\")).alias(col+\"_mean_odometer_encoding_diff\"),\n",
    "                (pl.col(col+\"_median_elapsed_years_encoding\") - pl.col(\"elapsed_years\")).alias(col+\"_median_elapsed_years_encoding_diff\"),\n",
    "                (pl.col(col+\"_mean_elapsed_years_encoding\") - pl.col(\"elapsed_years\")).alias(col+\"_mean_elapsed_years_encoding_diff\"),\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        X_valid = X_valid.join(fillna_map, on=col, how=\"left\")\n",
    "        X_valid = X_valid.with_columns(\n",
    "            [\n",
    "                (pl.col(col+\"_median_odometer_encoding\") - pl.col(\"odometer\")).alias(col+\"_median_odometer_encoding_diff\"),\n",
    "                (pl.col(col+\"_mean_odometer_encoding\") - pl.col(\"odometer\")).alias(col+\"_mean_odometer_encoding_diff\"),\n",
    "                (pl.col(col+\"_median_elapsed_years_encoding\") - pl.col(\"elapsed_years\")).alias(col+\"_median_elapsed_years_encoding_diff\"),\n",
    "                (pl.col(col+\"_mean_elapsed_years_encoding\") - pl.col(\"elapsed_years\")).alias(col+\"_mean_elapsed_years_encoding_diff\"),\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        test_df = test_df.join(fillna_map, on=col, how=\"left\")\n",
    "        test_df = test_df.with_columns(\n",
    "            [\n",
    "                (pl.col(col+\"_median_odometer_encoding\") - pl.col(\"odometer\")).alias(col+\"_median_odometer_encoding_diff\"),\n",
    "                (pl.col(col+\"_mean_odometer_encoding\") - pl.col(\"odometer\")).alias(col+\"_mean_odometer_encoding_diff\"),\n",
    "                (pl.col(col+\"_median_elapsed_years_encoding\") - pl.col(\"elapsed_years\")).alias(col+\"_median_elapsed_years_encoding_diff\"),\n",
    "                (pl.col(col+\"_mean_elapsed_years_encoding\") - pl.col(\"elapsed_years\")).alias(col+\"_mean_elapsed_years_encoding_diff\"),\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    value, bins = pd.cut(X_train[\"year\"], bins=20, labels=False, retbins=True)\n",
    "    X_train = X_train.with_columns(\n",
    "        pl.Series(value).cast(pl.Float64).alias(\"year_map\")\n",
    "    )\n",
    "    value = pd.cut(X_valid[\"year\"], bins=bins, labels=False)\n",
    "    X_valid = X_valid.with_columns(\n",
    "        pl.Series(value).cast(pl.Float64).alias(\"year_map\")\n",
    "    )\n",
    "    value = pd.cut(test_df[\"year\"], bins=bins, labels=False)\n",
    "    test_df= test_df.with_columns(\n",
    "        pl.Series(value).cast(pl.Float64).alias(\"year_map\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    value, bins = pd.cut(X_train[\"odometer\"], bins=20, labels=False, retbins=True)\n",
    "    X_train = X_train.with_columns(\n",
    "        pl.Series(value).cast(pl.Float64).alias(\"odometer_map\")\n",
    "    )\n",
    "    value = pd.cut(X_valid[\"odometer\"], bins=bins, labels=False)\n",
    "    X_valid = X_valid.with_columns(\n",
    "        pl.Series(value).cast(pl.Float64).alias(\"odometer_map\")\n",
    "    )\n",
    "    value = pd.cut(test_df[\"odometer\"], bins=bins, labels=False)\n",
    "    test_df= test_df.with_columns(\n",
    "        pl.Series(value).cast(pl.Float64).alias(\"odometer_map\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    cross_features = [\n",
    "        'region', 'year_map', 'manufacturer', 'condition', 'cylinders','fuel', 'odometer_map', 'title_status', 'transmission', 'drive', 'size',\n",
    "        'type', 'paint_color', 'state'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for i, col1 in enumerate(cross_features):\n",
    "        for col2 in cross_features[i+1:]:\n",
    "            tmp = X_train.groupby([col1, col2]).agg(pl.col(\"price\").mean()).rename({\"price\":f\"{col1}*{col2}_price\"})\n",
    "            X_train = X_train.join(tmp, on=[col1, col2], how=\"left\")\n",
    "            X_valid = X_valid.join(tmp, on=[col1, col2], how=\"left\")\n",
    "            test_df = test_df.join(tmp, on=[col1, col2], how=\"left\")\n",
    "            \n",
    "            \n",
    "    for cols in combinations(cross_features, 3):\n",
    "        group_cols = list(cols)  # Convert the combination tuple to a list\n",
    "        tmp = X_train.groupby(group_cols).agg(pl.col(\"price\").mean()).rename({\"price\":f\"{group_cols[0]}*{group_cols[1]}*{group_cols[2]}_price\"})\n",
    "        X_train = X_train.join(tmp, on=group_cols, how=\"left\")\n",
    "        X_valid = X_valid.join(tmp, on=group_cols, how=\"left\")\n",
    "        test_df = test_df.join(tmp, on=group_cols, how=\"left\")\n",
    "        \n",
    "        \n",
    "    # OrdinalEncoder: これはfoldごとではなくともよい\n",
    "    oe = OrdinalEncoder(categories=\"auto\",\n",
    "                        handle_unknown=\"use_encoded_value\",\n",
    "                        unknown_value=-2,\n",
    "                        encoded_missing_value=-1, # QUESTION: 欠損値は-1に変換する -> NaNに??\n",
    "                        )\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train, columns=X_train.columns, dtype=)\n",
    "    X_valid = pd.DataFrame(X_valid, columns=X_valid.columns)\n",
    "    test_df = pd.DataFrame(test_df, columns=test_df.columns)\n",
    "    CFG.categorical_features_ = [feature + \"_category\" for feature in CFG.categorical_features]\n",
    "    X_train[CFG.categorical_features_] = oe.fit_transform(X_train[CFG.categorical_features].values)\n",
    "    X_valid[CFG.categorical_features_] = oe.transform(X_valid[CFG.categorical_features].values)\n",
    "    test_df[CFG.categorical_features_] = oe.transform(test_df[CFG.categorical_features].values)\n",
    "    return X_train, X_valid, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, test_df = preprocessing_per_fold(CFG, train, test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.use_features = [\n",
    "    'odometer', 'year', 'drive*size*paint_color_price', 'title_status*transmission_price', 'manufacturer*condition_price', 'condition_max_encoding', \n",
    "    'condition_min_elapsed_years_encoding', 'year_map*cylinders*drive_price', 'fuel*title_status*size_price', 'manufacturer*odometer_map_price', \n",
    "    'transmission*drive*paint_color_price', 'condition_median_elapsed_years_encoding', 'transmission_max_encoding', 'size_category', 'size*type_price',\n",
    "    'odometer_map*drive*type_price', 'year_map*title_status*drive_price', 'fuel*drive*paint_color_price', 'cylinders*drive*type_price',\n",
    "    'condition*type_price', 'year_map*odometer_map_price', 'condition*cylinders_price', 'fuel*title_status*paint_color_price'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [col for col in CFG.use_features if \"_category\" in col]\n",
    "lgb_train = lgb.Dataset(X_train[CFG.use_features], X_train[\"price\"], categorical_feature = categorical_features,)\n",
    "lgb_valid = lgb.Dataset(X_valid[CFG.use_features], X_valid[\"price\"], categorical_feature = categorical_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24090 entries, 0 to 24089\n",
      "Columns: 745 entries, id to paint_color_category\n",
      "dtypes: float64(12), object(733)\n",
      "memory usage: 136.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: odometer: object, year: object, drive*size*paint_color_price: object, title_status*transmission_price: object, manufacturer*condition_price: object, condition_max_encoding: object, condition_min_elapsed_years_encoding: object, year_map*cylinders*drive_price: object, fuel*title_status*size_price: object, manufacturer*odometer_map_price: object, transmission*drive*paint_color_price: object, condition_median_elapsed_years_encoding: object, transmission_max_encoding: object, size*type_price: object, odometer_map*drive*type_price: object, year_map*title_status*drive_price: object, fuel*drive*paint_color_price: object, cylinders*drive*type_price: object, condition*type_price: object, year_map*odometer_map_price: object, condition*cylinders_price: object, fuel*title_status*paint_color_price: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m      2\u001b[0m                 lgb_param, \n\u001b[0;32m      3\u001b[0m                 lgb_train, \n\u001b[0;32m      4\u001b[0m                 valid_sets\u001b[39m=\u001b[39;49m[lgb_valid],\n\u001b[0;32m      5\u001b[0m                 categorical_feature \u001b[39m=\u001b[39;49m categorical_features,\n\u001b[0;32m      6\u001b[0m                 callbacks\u001b[39m=\u001b[39;49m[lgb\u001b[39m.\u001b[39;49mearly_stopping(stopping_rounds\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mstopping_rounds, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),],\n\u001b[0;32m      7\u001b[0m                 )\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\engine.py:245\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[0;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    247\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:3096\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3089\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_network(\n\u001b[0;32m   3090\u001b[0m         machines\u001b[39m=\u001b[39mmachines,\n\u001b[0;32m   3091\u001b[0m         local_listen_port\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mlocal_listen_port\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   3092\u001b[0m         listen_time_out\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m120\u001b[39m),\n\u001b[0;32m   3093\u001b[0m         num_machines\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mnum_machines\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   3094\u001b[0m     )\n\u001b[0;32m   3095\u001b[0m \u001b[39m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3096\u001b[0m train_set\u001b[39m.\u001b[39;49mconstruct()\n\u001b[0;32m   3097\u001b[0m \u001b[39m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3098\u001b[0m params\u001b[39m.\u001b[39mupdate(train_set\u001b[39m.\u001b[39mget_params())\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:2210\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2203\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2204\u001b[0m                 predictor\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predictor,\n\u001b[0;32m   2205\u001b[0m                 data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata,\n\u001b[0;32m   2206\u001b[0m                 used_indices\u001b[39m=\u001b[39mused_indices\n\u001b[0;32m   2207\u001b[0m             )\n\u001b[0;32m   2208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2209\u001b[0m     \u001b[39m# create train\u001b[39;00m\n\u001b[1;32m-> 2210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init(data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel, reference\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2211\u001b[0m                     weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, group\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup,\n\u001b[0;32m   2212\u001b[0m                     init_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_score, predictor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predictor,\n\u001b[0;32m   2213\u001b[0m                     feature_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_name, categorical_feature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_feature, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[0;32m   2214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:1801\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpandas_categorical \u001b[39m=\u001b[39m reference\u001b[39m.\u001b[39mpandas_categorical\n\u001b[0;32m   1800\u001b[0m     categorical_feature \u001b[39m=\u001b[39m reference\u001b[39m.\u001b[39mcategorical_feature\n\u001b[1;32m-> 1801\u001b[0m data, feature_name, categorical_feature, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpandas_categorical \u001b[39m=\u001b[39m _data_from_pandas(data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m   1802\u001b[0m                                                                                      feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[0;32m   1803\u001b[0m                                                                                      categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature,\n\u001b[0;32m   1804\u001b[0m                                                                                      pandas_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpandas_categorical)\n\u001b[0;32m   1806\u001b[0m \u001b[39m# process for args\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m params \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m params\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:698\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m feature_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    697\u001b[0m     feature_name \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m--> 698\u001b[0m _check_for_bad_pandas_dtypes(data\u001b[39m.\u001b[39;49mdtypes)\n\u001b[0;32m    699\u001b[0m df_dtypes \u001b[39m=\u001b[39m [dtype\u001b[39m.\u001b[39mtype \u001b[39mfor\u001b[39;00m dtype \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mdtypes]\n\u001b[0;32m    700\u001b[0m df_dtypes\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mfloat32)  \u001b[39m# so that the target dtype considers floats\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:661\u001b[0m, in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[1;34m(pandas_dtypes_series)\u001b[0m\n\u001b[0;32m    655\u001b[0m bad_pandas_dtypes \u001b[39m=\u001b[39m [\n\u001b[0;32m    656\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcolumn_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mpandas_dtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    657\u001b[0m     \u001b[39mfor\u001b[39;00m column_name, pandas_dtype \u001b[39min\u001b[39;00m pandas_dtypes_series\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    658\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype\u001b[39m.\u001b[39mtype)\n\u001b[0;32m    659\u001b[0m ]\n\u001b[0;32m    660\u001b[0m \u001b[39mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[1;32m--> 661\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mpandas dtypes must be int, float or bool.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    662\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFields with bad pandas dtypes: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(bad_pandas_dtypes)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: odometer: object, year: object, drive*size*paint_color_price: object, title_status*transmission_price: object, manufacturer*condition_price: object, condition_max_encoding: object, condition_min_elapsed_years_encoding: object, year_map*cylinders*drive_price: object, fuel*title_status*size_price: object, manufacturer*odometer_map_price: object, transmission*drive*paint_color_price: object, condition_median_elapsed_years_encoding: object, transmission_max_encoding: object, size*type_price: object, odometer_map*drive*type_price: object, year_map*title_status*drive_price: object, fuel*drive*paint_color_price: object, cylinders*drive*type_price: object, condition*type_price: object, year_map*odometer_map_price: object, condition*cylinders_price: object, fuel*title_status*paint_color_price: object"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "                lgb_param, \n",
    "                lgb_train, \n",
    "                valid_sets=[lgb_valid],\n",
    "                categorical_feature = categorical_features,\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=CFG.stopping_rounds, verbose=False),],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  evaluate\n",
    "# ===================================================================\n",
    "def train_lgb(CFG, lgb_param):\n",
    "    oof_df = pl.DataFrame()\n",
    "    preds = []\n",
    "    for fold in range(CFG.n_splits):\n",
    "        X_train, X_valid, test_df = preprocessing_per_fold(CFG, train, test, fold)\n",
    "        # train\n",
    "        categorical_features = [col for col in CFG.use_features if \"_category\" in col]\n",
    "        lgb_train = lgb.Dataset(X_train[CFG.use_features], X_train[\"price\"], categorical_feature = categorical_features,)\n",
    "        lgb_valid = lgb.Dataset(X_valid[CFG.use_features], X_valid[\"price\"], categorical_feature = categorical_features,)\n",
    "        model = lgb.train(\n",
    "                        lgb_param, \n",
    "                        lgb_train, \n",
    "                        valid_sets=[lgb_valid],\n",
    "                        categorical_feature = categorical_features,\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=CFG.stopping_rounds, verbose=False),],\n",
    "                        )\n",
    "        X_valid = X_valid.with_columns(\n",
    "            pl.Series(model.predict(X_valid[CFG.use_features], num_iteration=model.best_iteration)).alias(\"pred\")\n",
    "        )\n",
    "        print(f\"fold{fold}:\", get_score(y_true=X_valid[\"price\"], y_pred=X_valid[\"pred\"]))\n",
    "        oof_df = pl.concat([oof_df, X_valid])\n",
    "        preds.append(model.predict(test_df[CFG.use_features], num_iteration=model.best_iteration))\n",
    "    test_df = test_df.with_columns(\n",
    "        pl.Series(np.mean(preds, axis=0))\n",
    "    )\n",
    "    score = get_score(oof_df[\"price\"], oof_df[\"pred\"])\n",
    "    return score, oof_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Wrong type(Series) for label.\nIt should be list, numpy 1-D array or pandas Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 37\u001b[0m\n\u001b[0;32m     12\u001b[0m CFG\u001b[39m.\u001b[39muse_features \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m CFG\u001b[39m.\u001b[39muse_features \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_category\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m col]\n\u001b[0;32m     15\u001b[0m lgb_param \u001b[39m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mverbosity\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,   \n\u001b[0;32m     35\u001b[0m }\n\u001b[1;32m---> 37\u001b[0m best_score, oof_df, test_df \u001b[39m=\u001b[39m train_lgb(CFG, lgb_param)\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[32m\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m====== CV score ======\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[32m\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mbest_score\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 13\u001b[0m, in \u001b[0;36mtrain_lgb\u001b[1;34m(CFG, lgb_param)\u001b[0m\n\u001b[0;32m     11\u001b[0m lgb_train \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_train[CFG\u001b[39m.\u001b[39muse_features], X_train[\u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m], categorical_feature \u001b[39m=\u001b[39m categorical_features,)\n\u001b[0;32m     12\u001b[0m lgb_valid \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_valid[CFG\u001b[39m.\u001b[39muse_features], X_valid[\u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m], categorical_feature \u001b[39m=\u001b[39m categorical_features,)\n\u001b[1;32m---> 13\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m     14\u001b[0m                 lgb_param, \n\u001b[0;32m     15\u001b[0m                 lgb_train, \n\u001b[0;32m     16\u001b[0m                 valid_sets\u001b[39m=\u001b[39;49m[lgb_valid],\n\u001b[0;32m     17\u001b[0m                 categorical_feature \u001b[39m=\u001b[39;49m categorical_features,\n\u001b[0;32m     18\u001b[0m                 callbacks\u001b[39m=\u001b[39;49m[lgb\u001b[39m.\u001b[39;49mearly_stopping(stopping_rounds\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mstopping_rounds, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),],\n\u001b[0;32m     19\u001b[0m                 )\n\u001b[0;32m     20\u001b[0m X_valid \u001b[39m=\u001b[39m X_valid\u001b[39m.\u001b[39mwith_columns(\n\u001b[0;32m     21\u001b[0m     pl\u001b[39m.\u001b[39mSeries(model\u001b[39m.\u001b[39mpredict(X_valid[CFG\u001b[39m.\u001b[39muse_features], num_iteration\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mbest_iteration))\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfold\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m, get_score(y_true\u001b[39m=\u001b[39mX_valid[\u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m], y_pred\u001b[39m=\u001b[39mX_valid[\u001b[39m\"\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\engine.py:245\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[0;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    247\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:3096\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3089\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_network(\n\u001b[0;32m   3090\u001b[0m         machines\u001b[39m=\u001b[39mmachines,\n\u001b[0;32m   3091\u001b[0m         local_listen_port\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mlocal_listen_port\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   3092\u001b[0m         listen_time_out\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m120\u001b[39m),\n\u001b[0;32m   3093\u001b[0m         num_machines\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mnum_machines\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   3094\u001b[0m     )\n\u001b[0;32m   3095\u001b[0m \u001b[39m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3096\u001b[0m train_set\u001b[39m.\u001b[39;49mconstruct()\n\u001b[0;32m   3097\u001b[0m \u001b[39m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3098\u001b[0m params\u001b[39m.\u001b[39mupdate(train_set\u001b[39m.\u001b[39mget_params())\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:2210\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2203\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2204\u001b[0m                 predictor\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predictor,\n\u001b[0;32m   2205\u001b[0m                 data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata,\n\u001b[0;32m   2206\u001b[0m                 used_indices\u001b[39m=\u001b[39mused_indices\n\u001b[0;32m   2207\u001b[0m             )\n\u001b[0;32m   2208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2209\u001b[0m     \u001b[39m# create train\u001b[39;00m\n\u001b[1;32m-> 2210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init(data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel, reference\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2211\u001b[0m                     weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, group\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup,\n\u001b[0;32m   2212\u001b[0m                     init_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_score, predictor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predictor,\n\u001b[0;32m   2213\u001b[0m                     feature_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_name, categorical_feature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_feature, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[0;32m   2214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:1875\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1873\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot initialize Dataset from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   1874\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1875\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_label(label)\n\u001b[0;32m   1876\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_label() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLabel should not be None\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:2606\u001b[0m, in \u001b[0;36mDataset.set_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   2604\u001b[0m     label_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mravel(label)\n\u001b[0;32m   2605\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2606\u001b[0m     label_array \u001b[39m=\u001b[39m _list_to_1d_numpy(label, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m   2607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_field(\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m, label_array)\n\u001b[0;32m   2608\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_field(\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# original values can be modified at cpp side\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Komiyama Yuji\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:304\u001b[0m, in \u001b[0;36m_list_to_1d_numpy\u001b[1;34m(data, dtype, name)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(data, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)  \u001b[39m# SparseArray should be supported as well\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong type(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) for \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mIt should be list, numpy 1-D array or pandas Series\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Wrong type(Series) for label.\nIt should be list, numpy 1-D array or pandas Series"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  evaluate\n",
    "# ===================================================================\n",
    "CFG.use_features = [\n",
    "    'odometer', 'year', 'drive*size*paint_color_price', 'title_status*transmission_price', 'manufacturer*condition_price', 'condition_max_encoding', \n",
    "    'condition_min_elapsed_years_encoding', 'year_map*cylinders*drive_price', 'fuel*title_status*size_price', 'manufacturer*odometer_map_price', \n",
    "    'transmission*drive*paint_color_price', 'condition_median_elapsed_years_encoding', 'transmission_max_encoding', 'size_category', 'size*type_price',\n",
    "    'odometer_map*drive*type_price', 'year_map*title_status*drive_price', 'fuel*drive*paint_color_price', 'cylinders*drive*type_price',\n",
    "    'condition*type_price', 'year_map*odometer_map_price', 'condition*cylinders_price', 'fuel*title_status*paint_color_price'\n",
    "]\n",
    "\n",
    "CFG.use_features = [col for col in CFG.use_features if \"_category\" not in col]\n",
    "\n",
    "\n",
    "lgb_param = {\n",
    "    \"task\":\"train\",\n",
    "    \"objective\": \"mape\",\n",
    "    \"boosting\":\"gbdt\",\n",
    "    \"n_estimators\": 2506,\n",
    "    \"learning_rate\": 0.011165706106193283,\n",
    "    \"max_depth\": 13,\n",
    "    \"num_leaves\": 4371,\n",
    "    \"min_data_in_leaf\": 192,\n",
    "    \"max_bin\": 378,\n",
    "    \"subsample\": 0.9989161453457421,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"feature_fraction\": 0.21529074165670292,\n",
    "    \"reg_lambda\": 4.8708700129192055e-06,\n",
    "    \"reg_alpha\": 1.9865145426739786e-06,\n",
    "    \"scale_pos_weight\": 0.0010821916071735351,\n",
    "    \"num_threads\":CFG.num_cores,\n",
    "    \"metric\": 'mape',\n",
    "    \"seed\" : CFG.seed,\n",
    "    \"verbosity\": -1,   \n",
    "}\n",
    "\n",
    "best_score, oof_df, test_df = train_lgb(CFG, lgb_param)\n",
    "print('\\033[32m'+\"====== CV score ======\"+'\\033[0m')\n",
    "print('\\033[32m'+f'{best_score}'+'\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
